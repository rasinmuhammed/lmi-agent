name: Data Ingestion Pipeline

on:
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      search_terms:
        description: 'Job search terms (comma-separated)'
        required: false
        default: 'Machine Learning Engineer,Data Scientist,MLOps Engineer'

jobs:
  scrape-and-ingest:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        working-directory: ./backend
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Fetch jobs from API
        working-directory: ./backend
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
        run: |
          python scripts/ingest_data.py api \
            --search-terms ${{ github.event.inputs.search_terms || 'Machine Learning Engineer' }} \
            --api-source remoteok

      - name: Generate statistics
        working-directory: ./backend
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          python scripts/ingest_data.py stats

      - name: Notify on failure
        if: failure()
        run: |
          echo "Data pipeline failed! Check logs for details."
          # Add notification logic here (e.g., Slack, Email)